<h1 align="center">ğŸš€ ScaleCast</h1>

<p align="center">
  <strong>End-to-End MLOps Pipeline for Demand Forecasting</strong>
</p>

<p align="center">
  <img src="https://img.shields.io/badge/Python-3.10+-3776AB?style=for-the-badge&logo=python&logoColor=white" alt="Python" />
  <img src="https://img.shields.io/badge/Docker-2496ED?style=for-the-badge&logo=docker&logoColor=white" alt="Docker" />
  <img src="https://img.shields.io/badge/Airflow-017CEE?style=for-the-badge&logo=apache-airflow&logoColor=white" alt="Airflow" />
  <img src="https://img.shields.io/badge/AWS-232F3E?style=for-the-badge&logo=amazon-aws&logoColor=white" alt="AWS" />
  <img src="https://img.shields.io/badge/PostgreSQL-316192?style=for-the-badge&logo=postgresql&logoColor=white" alt="PostgreSQL" />
  <img src="https://img.shields.io/badge/GitHub_Actions-2088FF?style=for-the-badge&logo=github-actions&logoColor=white" alt="GitHub Actions" />
</p>

---

## ğŸ“ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          ScaleCast MLOps Pipeline                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                                 â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚   S3    â”‚â”€â”€â”€â”€â”€â–¶â”‚      Great      â”‚â”€â”€â”€â”€â”€â–¶â”‚ PostgreSQL â”‚â”€â”€â”€â”€â”€â–¶â”‚   ML    â”‚   â”‚
â”‚   â”‚  (raw)  â”‚      â”‚  Expectations   â”‚      â”‚ (warehouse)â”‚      â”‚Training â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚  (validation)   â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜   â”‚
â”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                               â”‚        â”‚
â”‚                            â”‚                                         â–¼        â”‚
â”‚                            â”‚ âŒ Circuit                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚                            â”‚    Breaker                        â”‚   S3    â”‚    â”‚
â”‚                            â–¼                                   â”‚(models) â”‚    â”‚
â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                           â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜    â”‚
â”‚                    â”‚  Alert/Stop   â”‚                                â”‚         â”‚
â”‚                    â”‚   Pipeline    â”‚                                â–¼         â”‚
â”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚                                                               â”‚ FastAPI â”‚     â”‚
â”‚                                                               â”‚(serving)â”‚     â”‚
â”‚                                                               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                                                                                 â”‚
â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”‚
â”‚                    â”‚         Apache Airflow              â”‚                     â”‚
â”‚                    â”‚      (Orchestration Layer)          â”‚                     â”‚
â”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚
â”‚                                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## âœ¨ Key Features

- ğŸ›¡ï¸ **Automated Data Validation** â€” Circuit breaker pattern stops pipeline on bad data
- ğŸ“¦ **Data Versioning** â€” Track datasets with DVC for reproducibility
- âš™ï¸ **Workflow Orchestration** â€” Apache Airflow schedules and monitors pipelines
- ğŸŒ **Model Serving** â€” FastAPI provides low-latency prediction endpoints
- ğŸ”„ **CI/CD Pipeline** â€” GitHub Actions for linting, testing, and Docker builds
- ğŸ³ **Infrastructure as Code** â€” Fully containerized with Docker Compose

---

## ğŸ› ï¸ Tech Stack

| Component | Technology | Purpose |
|-----------|------------|---------|
| **Orchestration** | Apache Airflow 2.7 | Workflow scheduling and monitoring |
| **Data Validation** | Great Expectations | Schema and quality checks |
| **Database** | PostgreSQL 15 | Data warehouse for training data |
| **ML Framework** | scikit-learn | Random Forest demand forecasting |
| **API** | FastAPI 0.104 | Model serving with auto-generated docs |
| **Cloud Storage** | AWS S3 | Artifact and model storage |
| **Version Control** | DVC 3.30 | Data and model versioning |
| **CI/CD** | GitHub Actions | Automated testing and builds |
| **Containerization** | Docker Compose | Local development environment |

---

## ğŸ“ Project Structure

```
scalecast/
â”œâ”€â”€ ğŸ“‚ airflow/
â”‚   â”œâ”€â”€ dags/                   # Airflow DAG definitions
â”‚   â”‚   â””â”€â”€ scalecast_pipeline.py
â”‚   â”œâ”€â”€ logs/                   # Airflow logs (gitignored)
â”‚   â””â”€â”€ plugins/                # Custom Airflow plugins
â”œâ”€â”€ ğŸ“‚ configs/
â”‚   â””â”€â”€ config.yaml             # Central configuration
â”œâ”€â”€ ğŸ“‚ data/
â”‚   â”œâ”€â”€ raw/                    # Raw input data (DVC tracked)
â”‚   â””â”€â”€ processed/              # Processed datasets
â”œâ”€â”€ ğŸ“‚ models/                  # Trained model artifacts
â”œâ”€â”€ ğŸ“‚ scripts/
â”‚   â”œâ”€â”€ setup_aws.py            # AWS S3 bucket setup
â”‚   â”œâ”€â”€ generate_keys.py        # Generate Fernet keys
â”‚   â””â”€â”€ init_db.sql             # Database schema initialization
â”œâ”€â”€ ğŸ“‚ src/
â”‚   â”œâ”€â”€ api/                    # FastAPI application
â”‚   â”‚   â””â”€â”€ main.py
â”‚   â”œâ”€â”€ data_ingestion/         # Data loading utilities
â”‚   â”œâ”€â”€ data_validation/        # Great Expectations checks
â”‚   â”‚   â””â”€â”€ validate_demand_data.py
â”‚   â””â”€â”€ training/               # Model training pipeline
â”œâ”€â”€ ğŸ“‚ tests/                   # Test suite
â”‚   â””â”€â”€ test_validation.py
â”œâ”€â”€ ğŸ“‚ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â””â”€â”€ ci.yml              # GitHub Actions CI pipeline
â”œâ”€â”€ docker-compose.yml          # Docker services definition
â”œâ”€â”€ Dockerfile.airflow          # Airflow container
â”œâ”€â”€ Dockerfile.api              # FastAPI container
â”œâ”€â”€ requirements.txt            # Python dependencies
â””â”€â”€ README.md
```

---

## ğŸš€ Quick Start

### 1. Clone the Repository

```bash
git clone https://github.com/yourusername/scalecast.git
cd scalecast
```

### 2. Configure Environment

```bash
# Copy the example environment file
cp .env.example .env

# Edit .env with your configuration
```

### 3. Add AWS Credentials

Add these to your `.env` file:

```bash
AWS_ACCESS_KEY_ID=your_access_key
AWS_SECRET_ACCESS_KEY=your_secret_key
AWS_REGION=us-east-1
S3_BUCKET_NAME=your-bucket-name
```

### 4. Start Services

```bash
# Start all services (PostgreSQL, Airflow Webserver, Scheduler)
docker-compose up -d

# Check service status
docker-compose ps
```

### 5. Access Airflow UI

Open [http://localhost:8080](http://localhost:8080) in your browser.

| Field | Value |
|-------|-------|
| Username | `admin` |
| Password | `admin` |

### 6. Trigger the Pipeline

1. Navigate to **DAGs** in the Airflow UI
2. Find `scalecast_demand_pipeline`
3. Toggle the DAG to **On**
4. Click the **Play** button to trigger manually

---

## ğŸ“Š Pipeline Overview

The `scalecast_demand_pipeline` DAG executes four sequential tasks:

| Task | Description |
|------|-------------|
| **validate_data** | Validates raw CSV against schema and business rules. Implements circuit breaker â€” pipeline stops if validation fails. |
| **load_to_postgres** | Loads validated data into `warehouse.demand_data` table. Truncates existing data before insert. |
| **train_model** | Trains Random Forest regressor with feature engineering (day of week, month, weekend flag). Outputs MAE, RMSE, RÂ² metrics. |
| **upload_model** | Uploads `demand_model.pkl` and `encoders.pkl` to S3 for model serving. |

```
validate_data  â”€â”€â–¶  load_to_postgres  â”€â”€â–¶  train_model  â”€â”€â–¶  upload_model
```

---

## ğŸ”Œ API Usage

### Health Check

```bash
curl http://localhost:8000/health
```

**Response:**
```json
{
  "status": "healthy",
  "model_loaded": true
}
```

### Make a Prediction

```bash
curl -X POST http://localhost:8000/predict \
  -H "Content-Type: application/json" \
  -d '{
    "date": "2024-03-15",
    "store_id": "STORE_001",
    "product_id": "PROD_A",
    "price": 29.99,
    "promotion": true
  }'
```

**Response:**
```json
{
  "prediction": 142.5,
  "model_version": "1.0.0"
}
```

### API Documentation

Interactive Swagger docs available at [http://localhost:8000/docs](http://localhost:8000/docs)

---

## ğŸ§ª Running Tests

```bash
# Create virtual environment
python -m venv venv
source venv/bin/activate  # Unix/macOS
# venv\Scripts\activate   # Windows

# Install dependencies
pip install -r requirements.txt

# Run tests with verbose output
pytest tests/ -v
```

**Expected output:**
```
tests/test_validation.py::test_validation_passes_with_valid_data PASSED
tests/test_validation.py::test_validation_fails_with_missing_column PASSED
tests/test_validation.py::test_validation_fails_with_null_values PASSED
```

---

## ğŸ§© Core Engineering Challenges Addressed

| Challenge | Solution |
|-----------|----------|
| **Reproducibility** | DVC links trained models to exact training data versions, enabling rollback and audit |
| **Data Quality** | Great Expectations acts as a circuit breaker â€” bad data stops the pipeline before corrupting models |
| **Decoupling** | S3 serves as middleware between training and serving, allowing independent scaling |
| **Automation** | Airflow orchestrates the entire pipeline with scheduling, retries, and dependency management |

---

## ğŸ”® Future Improvements

- ğŸ“ˆ **MLflow Integration** â€” Add experiment tracking and model registry
- ğŸ”¬ **Model A/B Testing** â€” Implement traffic splitting for model comparison
- ğŸ“Š **Grafana Dashboards** â€” Add monitoring for pipeline metrics and model performance
- â˜¸ï¸ **AWS ECS/EKS Deployment** â€” Migrate to managed container orchestration for production

---

## ğŸ“„ License

This project is licensed under the MIT License â€” see the [LICENSE](LICENSE) file for details.

---

<p align="center">
  <strong>ScaleCast</strong> â€” Built for scalable demand forecasting ğŸ“Š
</p>
